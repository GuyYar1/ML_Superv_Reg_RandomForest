{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f2b8c9-8562-47fa-bff2-47af0f6df444",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'firebase_admin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfirebase_admin\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'firebase_admin'"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "import firebase_admin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from utils.INI_Utility import *\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pathlib import Path\n",
    "from firebase_admin import db\n",
    "from firebase_admin import credentials\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def download_from_gdrive(url, filename):\n",
    "    # Extract the file ID from the URL\n",
    "    file_id = url.split('/')[-2]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Download the file\n",
    "    if Path(filename).exists():\n",
    "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "    else:\n",
    "        gdown.download(download_url, filename, quiet=False)\n",
    "        print(f\"File downloaded as: {filename}\")\n",
    "\n",
    "def get_df_Url():\n",
    "\n",
    "    ' the two url should be in config'\n",
    "    train_url = initialize_ini().get_value('DATASET', 'train_url')\n",
    "    valid_url = initialize_ini().get_value('DATASET', 'valid_url')\n",
    "    # Example usage\n",
    "\n",
    "    download_from_gdrive(train_url, 'train.csv')\n",
    "    download_from_gdrive(valid_url, 'valid.csv')\n",
    "\n",
    "    df_train = pd.read_csv('train.csv')\n",
    "    df_valid = pd.read_csv('valid.csv')\n",
    "\n",
    "    print(df_train.head())\n",
    "    print(df_valid.head())\n",
    "\n",
    "    return df_train, df_valid\n",
    "\n",
    "\n",
    "def get_df_sns():\n",
    "    name = initialize_ini().get_value('DATASET', 'sns_name')\n",
    "    df_all = sns.load_dataset(name)  # ('tips')\n",
    "    return df_all, None\n",
    "\n",
    "\n",
    "def get_df(url_en):\n",
    "    \"\"\"\n",
    "    url_en = True so retrive df from url_name\n",
    "    url_en = False so retrive df from sns name using url_name\n",
    "    \"\"\"\n",
    "    if url_en:\n",
    "        return get_df_Url()\n",
    "    else:\n",
    "        return get_df_sns()\n",
    "\n",
    "\n",
    "def initialize_ini():\n",
    "\n",
    "    \"\"\"        \n",
    "    :return: \n",
    "    \"\"\"\"\"\" Get value from section 'TRAIN'\n",
    "    learning_rate = ini_util.get_value('TRAIN', 'url')\n",
    "     Set value in section 'VALID'\n",
    "    ini_util.set_value('VALID', 'url', '200')\n",
    "    ini_util.save_changes()\n",
    "    \"\"\"\n",
    "    ini_file = \"config.INI\"\n",
    "    ini_util = SingletonINIUtility(ini_file)\n",
    "    ini_util.read_ini()\n",
    "    return ini_util\n",
    "\n",
    "def train(model, X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    print(\"_____CREATE  train_test_split USING TEST SIZE, with random tree state\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Train the model on the training set\n",
    "    print(\"_______Perform fit to learn from X train and y train______\")\n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "\n",
    "    print(\" start model.fit \")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\" End model.fit \")\n",
    "    # Get the best hyperparameters\n",
    "    # best_params = model.best_params_\n",
    "    # print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    # Access model attributes\n",
    "    feature_importances = model.feature_importances_\n",
    "    feature_names = model.feature_names_in_\n",
    "    print(\"-----------feature_importances-----------------\")\n",
    "    # Print the results\n",
    "    print(pd.Series(feature_importances, index=feature_names).sort_values(ascending=False))\n",
    "    print(\"________________________________\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def trainr_pca(model, X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    pca = PCA(n_components=4)  # Keep 2 components\n",
    "    print(\"_____CREATE  train_test_split USING TEST SIZE, with random tree state\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_reduced = pca.transform(X_train)\n",
    "    pca.fit(X_train)\n",
    "    X_train_reduced = pca.transform(X_train)\n",
    "    X_test_reduced = pca.transform(X_test)\n",
    "    # Train the model on the training set\n",
    "    print(\"_______Perform fit to learn from X train and y train______\")\n",
    "    # Evaluate the model on the testing set\n",
    "    # Access model attributes\n",
    "    feature_importances = model.feature_importances_\n",
    "    feature_names = model.feature_names_in_\n",
    "    print(\"-----------feature_importances-----------------\")\n",
    "    # Print the results\n",
    "    print(pd.Series(feature_importances, index=feature_names).sort_values(ascending=False))\n",
    "    print(\"________________________________\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def RMSE(y_pred, y_true):\n",
    "    # Calculate the root mean squared error (RMSE)\n",
    "    return ((y_pred - y_true) ** 2).mean() ** 0.5\n",
    "\n",
    "\n",
    "def predict_y(model, X_train, X_test, y_train, y_test, pca, ver=1.0, subModel=1):\n",
    "    # Make predictions on the training and testing sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    a, train_rmse, test_rmse, raw_train_std, train_pred_std, test_std, xx = model_summary(\n",
    "                                                      pca, y_test, y_test_pred, y_train, y_train_pred, subModel)\n",
    "\n",
    "\n",
    "    a= float((get_int_from_ini('TRAIN', 'max_depth')))\n",
    "\n",
    "\n",
    "    dict_to_db = {\n",
    "          \"user\": os.getlogin(),\n",
    "          \"submodel\": subModel,\n",
    "          \"Ver\": ver,\n",
    "          \"Train without Model from raw data\": a,\n",
    "          \"train_rmse\": train_rmse,\n",
    "          \"test_rmse\": test_rmse,\n",
    "          \"raw_train_std\": raw_train_std,\n",
    "          \"train_pred_std\": train_pred_std,\n",
    "          \"test_std\": test_std,\n",
    "          \"max_depth\": float(get_int_from_ini('TRAIN', 'max_depth')),\n",
    "          \"min_samples_split\": float(get_int_from_ini('TRAIN', 'min_samples_split')),\n",
    "          \"min_samples_leaf\": float(get_int_from_ini('TRAIN', 'min_samples_leaf')),\n",
    "          \"n_estimators\": float(get_int_from_ini('TRAIN', 'n_estimators')),\n",
    "          \"max_features\": float(get_int_from_ini('TRAIN', 'max_features'))\n",
    "    }\n",
    "\n",
    "    # ref = create_firebase_admin()\n",
    "    # #clearfromdb(ref, ['-O1CamdoXgL3sG8aOW5G', 'O1CamszdCzx6mrlEkAu', '-O1CcvUk3rZD0iYCO76-','-O1CdFe49Ye4QqY9zW3J'])\n",
    "    #\n",
    "    # write_and_get_db(ref, dict_to_db)\n",
    "    # json_firbase = ref.get()\n",
    "    # json_firbase1 = pd.DataFrame(json_firbase['messages']).transpose()\n",
    "    # print(json_firbase1)\n",
    "\n",
    "    plt.plot(xx, xx, 'r--')\n",
    "    plt.xlabel('actual')\n",
    "    plt.ylabel('predicted')\n",
    "\n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "\n",
    "def create_firebase_admin():\n",
    "    return db.reference()\n",
    "\n",
    "\n",
    "def model_summary(pca, y_test, y_test_pred, y_train, y_train_pred, subModel):\n",
    "    print(\"____________Learning Metric result ____________________\")\n",
    "    print(\"train data is the data that created the model.\")\n",
    "    print(\"train data is the data that i only have. test data is not static and be changed\")\n",
    "    print(\"so when i have the model already after the fit inside the train function.\")\n",
    "    print(\"I use the model to predict the y_train from X train. same for the X,y test.\")\n",
    "    print(\"the model should have simmilar residue\\ error on prediction from test,train.\")\n",
    "    print(f\"Look below: with {pca}\")\n",
    "    a = round(y_train.mean(), 3)\n",
    "    b = round(RMSE(y_train_pred, y_train), 3)\n",
    "    c = round(RMSE(y_test_pred, y_test), 3)\n",
    "    d = round(y_train.std(), 3)\n",
    "    e = round(y_train_pred.std(), 3)\n",
    "    f = round(y_test.std(), 3)\n",
    "    print(f\"data was split to submodels. This is part: {subModel}\")\n",
    "    print(\"Train without Model from raw data. mean:\", a)\n",
    "    print(\"Train RMSE:\", b)\n",
    "    print(\"Test RMSE:\", c)\n",
    "    print(\"Raw Train STD\", d)\n",
    "    print(\"Test STD\", f)\n",
    "    print(\"Train_pred STD\", e)\n",
    "    print(\"Conclusions: \")\n",
    "    print(\"Train STD Vs. Test STD:\")\n",
    "    print(\"Train RMSE Vs. Test RMSE: RMSE should be similar but here the diff is  factors \")\n",
    "    print(\"RMSE/STD: focuses on prediction accuracy, while STD describes data variability.\")\n",
    "    print(\"RMSE: Visualizing learning curves or comparing RMSE across different models can provide insights\")\n",
    "    print(\"________________________________\")\n",
    "    print(\"scatterplot\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    print(\"-------------scatterplot--------> x=y_test, y=y_test_pred --\")\n",
    "    sns.scatterplot(x=y_test, y=y_test_pred)\n",
    "    xx = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "    return a, b, c, d, e, f, xx\n",
    "\n",
    "\n",
    "### Encoding\n",
    "def cut_encode(df):\n",
    "    # Map categorical variables to numeric values\n",
    "    cut_mapping = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
    "    # Apply the mapping to the 'cut' column\n",
    "    df['cut_encoded'] = df['cut'].map(cut_mapping)\n",
    "    # Drop the original 'cut' column\n",
    "    df = df.drop('cut', axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "### ###  ###\n",
    "\n",
    "def eda_analysis(df, learn_column, categ_heu, full=False):\n",
    "    # 5 rows in table\n",
    "    print(df.head())\n",
    "    #  rangeIndex, num column, dtype(float64,category int64\n",
    "    print(\"________________________________\")\n",
    "    print(\"-------------info--------> rangeIndex, num column, dtype(float64,category int64)--------\")\n",
    "    print(df.info())  # rangeIndex, num column, dtype(float64,category int64)\n",
    "    #  Category means non mumeric valus. i can have numeric values in category columns - Not good)\n",
    "    print(\"________________________________\")\n",
    "    print(\n",
    "        \"-------------describe--------> perform only for numeric values which has numeric dtype a statistical  view.--------\")\n",
    "    print(df.describe())  # perform only for numeric values which has numeric dtype a statistical  view.\n",
    "    print(\"Look here\")\n",
    "    print(\"-------------pairplot--------> show a plot of mix numeric values, can use hue as category distribution--------\")\n",
    "    sns.pairplot(df)\n",
    "    plt.show(block=True)  # Display the plot\n",
    "\n",
    "    #sns.pairplot(df, hue=categ_heu)  # show a plot of mix numeric values, can use hue as category distribution\n",
    "    #plt.show()  # Display the plot\n",
    "\n",
    "    print(\"-------------displot--------> visualize the distribution of tip amounts. kernel density estimate--------\")\n",
    "    sns.displot(data=df, x=learn_column, kde=True)  # visualize the distribution of tip amounts. kernel density estimate\n",
    "    plt.show(block=True)  # Display the plot\n",
    "    sns.displot(data=df, x='ModelID', kde=True)  # visualize the distribution of tip amounts. kernel density estimate\n",
    "    plt.show(block=True) # Display the plot\n",
    "    print(\n",
    "        \"-------------df.value_counts--------> for each column show you the distribution.  text and figure bar histogram--\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        print(df[col].value_counts())  # Show the distribution for each column\n",
    "        print()\n",
    "        sns.displot(data=df, x=col, kde=True)\n",
    "        plt.title(f'Bar Histogram for {col}')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "        plt.show()\n",
    "        print(\"________________________________\")\n",
    "\n",
    "def eda_post_analysis(y_train):\n",
    "    ### a little bit more EDA\n",
    "    print(\"____________Normal Dist____________________\")\n",
    "    print(\"how many samples are between +-std from the mean?\")\n",
    "    print(\"IF Normal Dist. so between 1 std there are 68% 0.68\")\n",
    "    print(\"IF Normal Dist. so between 2 std there are 95% 0.95\")\n",
    "    print(\"IF Normal Dist. so between 3 std there are 99.7% 0.997\")\n",
    "    print(\"____________Uniform Dist____________________\")\n",
    "    print(\"IF Uniform Dist. so between 1 std there are 50% 0.50\")\n",
    "    print(\"IF Uniform Dist. so between 2 std there are 95% 0.95\")\n",
    "    low = y_train.mean() - y_train.std()\n",
    "    high = y_train.mean() + y_train.std()\n",
    "\n",
    "    # Solution A\n",
    "    print(\"According to the % of samples between +-1 std. I can decide if it is Normal Dist. or UniForm:\")\n",
    "    print(\"       -->\" + str(len(y_train[(y_train >= low) & (y_train <= high)]) / len(y_train)))\n",
    "    print(\"sns.displot helps also to decide if Normal Dist. or Uniform or other\")\n",
    "\n",
    "    # solution B\n",
    "    ((y_train >= low) & (y_train <= high)).sum() / len(y_train)\n",
    "\n",
    "    # Solution C\n",
    "    ((y_train >= low) & (y_train <= high)).mean()\n",
    "\n",
    "    # Solution D\n",
    "    y_train.between(low, high).mean()\n",
    "\n",
    "    # Solution E\n",
    "    y_train[y_train.between(low, high)].mean()\n",
    "\n",
    "    # Solution F\n",
    "    y_train[(y_train >= low) & (y_train <= high)].mean()\n",
    "    print(\"________________________________\")\n",
    "\n",
    "\n",
    "def impute_nan(df, method='mean'):\n",
    "    \"\"\"\n",
    "    Impute NaN values in a DataFrame based on the specified method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        method (str): Imputation method ('mean', 'median', or 'constant').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with NaN values imputed.\n",
    "    \"\"\"\n",
    "    if method == 'mean':\n",
    "        imputed_values = df.mean()\n",
    "    elif method == 'median':\n",
    "        imputed_values = df.median()\n",
    "    elif method == 'constant':\n",
    "        imputed_values = 999  # Replace NaNs with a constant value (adjust as needed)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid imputation method. Choose 'mean', 'median', or 'constant'.\")\n",
    "\n",
    "    df.fillna(imputed_values, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(df, exe_missing=False, exe_nonnumeric_code=False, exe_exclusenonnumeric=False,\n",
    "                 exe_dropna=False, exe_dummies=False, exe_FromfilterYear=1001, print_info=False):\n",
    "    \"\"\"\n",
    "    Prepare data by handling missing values, converting non-numeric columns, excluding non-numeric columns,\n",
    "    dropping rows with missing values, and creating one-hot encoded columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        exe_missing (bool): Execute missing value handling.\n",
    "        exe_nonnumeric_code (bool): Execute non-numeric column conversion to codes.\n",
    "        exe_exclusenonnumeric (bool): Execute exclusion of non-numeric columns.\n",
    "        exe_dropna (bool): Execute dropping rows with missing values.\n",
    "        exe_dummies (bool): Execute one-hot encoding.\n",
    "        print_info (bool): Print DataFrame info at each step.\n",
    "        exe_FromfilterYear: start from which year to filter . predict future so need more releavnt data\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    df_orig = df.copy()\n",
    "    print(df_orig.head().T)\n",
    "\n",
    "    if exe_FromfilterYear > 0:\n",
    "        print(\"filter Year  DataFrame from:\", exe_FromfilterYear )\n",
    "        df = df[df['YearMade'] > exe_FromfilterYear]\n",
    "\n",
    "    if print_info:\n",
    "        print(\"Original DataFrame info:\")\n",
    "        print(df.info())\n",
    "\n",
    "    if exe_dropna:\n",
    "        print(\"exe_dropna\")\n",
    "        df = df.dropna()  # Remove rows with missing values\n",
    "        if print_info:\n",
    "            print(\"After dropping rows with missing values:\")\n",
    "            print(\"df_tmp.isna().sum()\", df.isna().sum())\n",
    "\n",
    "    if exe_nonnumeric_code:\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # Convert non-numeric columns to codes\n",
    "        for column in df_copy.select_dtypes(exclude=['number']).columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_copy[column]):\n",
    "                df_copy[column] = pd.Categorical(df_copy[column]).codes + 1\n",
    "\n",
    "        # Optionally, print information about the DataFrame\n",
    "        if print_info:\n",
    "            print(\"After converting non-numeric columns to codes:\")\n",
    "            print(df_copy.info())\n",
    "\n",
    "    if exe_missing:\n",
    "        print(\"exe_missing\")\n",
    "        df = handle_missing_values(df, action='missing_category') # it was IMPUTE and works but still\n",
    "        if print_info:\n",
    "            print(\"# Check to see how many examples were missing in `auctioneerID` column\")\n",
    "            print(df.value_counts())\n",
    "\n",
    "    if exe_dummies:\n",
    "        print(\"exe_dummies\")  # one-hot encoded\n",
    "        # Consider not using this with random forests\n",
    "        df = pd.get_dummies(df)  # Converts categorical variables into numerical representations\n",
    "        if print_info:\n",
    "            print(\"After creating one-hot encoded columns:\")\n",
    "            print(df.info())\n",
    "\n",
    "    if exe_exclusenonnumeric:\n",
    "        print(\"exe_exclusenonnumeric\")\n",
    "        df = df.select_dtypes(include='number')\n",
    "        if print_info:\n",
    "            print(\"if i ran exe_nonnumeric_code before so this should not have work to do\")\n",
    "            print(\"After excluding non-numeric columns:\")\n",
    "            print(df.info())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Temp due to error ValueError: could not convert string to float: 'Medium'\n",
    "    # Assuming 'UsageBand' is the column with 'Medium'\n",
    "    df['UsageBand'] = df['UsageBand'].replace('Medium', -1)\n",
    "    # Verify the updated values\n",
    "    print(df['UsageBand'].unique())\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, action='impute'):\n",
    "    \"\"\"\n",
    "    Handles missing values (NaNs) in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        action (str, optional): Action to perform ('impute' or 'missing_category').\n",
    "            Defaults to 'impute'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    if action == 'impute':\n",
    "        # Separate numeric and categorical columns\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "        # Impute missing values with mean for numeric columns\n",
    "        imputer = SimpleImputer(strategy='mean', add_indicator=False)\n",
    "        df_imputed_numeric = pd.DataFrame(imputer.fit_transform(df[numeric_cols]), columns=numeric_cols)\n",
    "\n",
    "        # Treat missing values as -1 for categorical columns\n",
    "        df_imputed_categorical = df[categorical_cols].fillna(-1)\n",
    "\n",
    "        # Combine the imputed numeric and categorical columns\n",
    "        df_imputed = pd.concat([df_imputed_numeric, df_imputed_categorical], axis=1)\n",
    "\n",
    "    elif action == 'missing_category':\n",
    "        # Treat missing values as -1 (for all columns)\n",
    "        df_imputed = df.fillna(-1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action. Choose 'impute' or 'missing_category'.\")\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "def clean_sigma_log(df, learn_column, clearedcolumn, cnt_std=3, method='sigma', column_with_long_tail='carat',\n",
    "                    mode=\"Validation\"):\n",
    "    \"\"\"\n",
    "      gENERAL TO BOTH : df, learn_column , method\n",
    "      sigma= clearedcolumn , cnt_std\n",
    "      log = column_with_long_tail\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard deviation for 'carat'\n",
    "\n",
    "    if mode == \"Validation\":\n",
    "        return df\n",
    "\n",
    "    if learn_column == column_with_long_tail:\n",
    "        print(\"error: Don't transform the target variable ('price'): Focus on transforming predictor variables\")\n",
    "        return df\n",
    "\n",
    "    if method == 'sigma':\n",
    "        mean = df[clearedcolumn].mean()\n",
    "        std = df[clearedcolumn].std()\n",
    "\n",
    "        # Define upper and lower bounds (3 standard deviations from the mean)\n",
    "        upper_bound = mean + cnt_std * std\n",
    "        lower_bound = mean - cnt_std * std\n",
    "\n",
    "        # Filter the DataFrame to keep data within the bounds\n",
    "        df_filtered = df[(df[clearedcolumn] >= lower_bound) & (df[clearedcolumn] <= upper_bound)]\n",
    "\n",
    "        print(\"Original DataFrame shape:\", df.shape)\n",
    "        print(\"Filtered DataFrame shape:\", df_filtered.shape, \"sigma\\std:\", cnt_std * 2)\n",
    "    elif method == 'log':\n",
    "        # Assuming 'df' is your DataFrame and 'column_with_long_tail' is the column you want to transform\n",
    "        df['transformed' + column_with_long_tail] = np.log(df[column_with_long_tail])\n",
    "        df[column_with_long_tail] = df['transformed' + column_with_long_tail]\n",
    "        df = df.drop('transformed' + column_with_long_tail, axis=1)\n",
    "        df_filtered = df\n",
    "        print(f\" column{column_with_long_tail} is after log transforming due to long right tail\")\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def build_model(rf_model, df, learn_column, pca, ver):\n",
    "    print(\"build_model\")\n",
    "    X = df.drop(columns=learn_column)  # these are our \"features\" that we use to predict from\n",
    "    y = df[learn_column]  # this is what we want to learn to predict\n",
    "\n",
    "    if pca:\n",
    "        X_train, X_test, y_train, y_test = trainr_pca(rf_model, X, y)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train(rf_model, X, y)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def columns_to_drop(X, skip=True, learn_column=None):\n",
    "\n",
    "    if skip:\n",
    "        return X\n",
    "\n",
    "    columntodrop = ['MachineID', 'auctioneerID', 'Backhoe_Mounting', 'Hydraulics', 'Pushblock',\n",
    "                                                                                            'Ripper',\n",
    "                                                                                            'Scarifier',\n",
    "                                                                                            'Tip_Control',\n",
    "                                                                                            'Tire_Size',\n",
    "                                                                                            'Coupler_System',\n",
    "                                                                                            'Grouser_Tracks',\n",
    "                                                                                            'Hydraulics_Flow',\n",
    "                                                                                            'Undercarriage_Pad_Width',\n",
    "                                                                                            'Stick_Length',\n",
    "                                                                                            'Thumb',\n",
    "                                                                                            'Pattern_Changer',\n",
    "                                                                                            'Grouser_Type']\n",
    "    check_col_exists_df(X, columntodrop)\n",
    "\n",
    "    X = X.drop(columns=columntodrop)\n",
    "    return X\n",
    "\n",
    "\n",
    "def check_col_exists_df(X, columntodrop):\n",
    "    try:\n",
    "        # Check if each column exists in the DataFrame\n",
    "        for col in columntodrop:\n",
    "            if col not in X.columns:\n",
    "                print(f\"Column '{col}' doesn't exist in the DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def ColumnsToKeep(X, skip=True, learn_column=None):\n",
    "\n",
    "    print(\"column to Keep\")\n",
    "    if skip:\n",
    "        return X\n",
    "\n",
    "    # Assuming X is your DataFrame\n",
    "    columns_to_keep1 = ['SalesID', 'YearMade', 'range_min', 'HandNum', 'saleYear_y', 'saleMonth', 'saleDay',\n",
    "                       'saleDayofweek', 'saleDayofyear']\n",
    "    columns_to_keep2 = ['SalesID', 'YearMade', 'range_min', 'HandNum', 'saleYear_y', 'saleMonth', 'saleDay',\n",
    "    'saleDayofweek', 'saleDayofyear',\n",
    "    'datasource', 'auctioneerID', 'MachineHoursCurrentMeter',\n",
    "    'UsageBand', 'saledate', 'fiModelDesc',\n",
    "    'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer',\n",
    "    'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls',\n",
    "    'Differential_Type', 'Steering_Controls']\n",
    "\n",
    "    columns_to_keep = columns_to_keep2\n",
    "\n",
    "    #, 'InteractionFeature', 'Decade', 'LogMachineHours']\n",
    "\n",
    "    if not(learn_column is None):\n",
    "        columns_to_keep.append(learn_column)\n",
    "\n",
    "    # Alternatively, you can use the drop method to achieve the same result\n",
    "    X = X.drop(columns=[col for col in X.columns if col not in columns_to_keep])\n",
    "    return X\n",
    "\n",
    "\n",
    "def predict_with_model(X_train, X_test, y_train, y_test, rf_model, ver, pca, subModel):\n",
    "\n",
    "    if pca:\n",
    "        pass  # TBD\n",
    "    else:\n",
    "        return predict_y(rf_model, X_train, X_test, y_train, y_test, pca, ver, subModel)\n",
    "\n",
    "\n",
    "def get_bool_from_ini(section, key):\n",
    "    return ini_util.get_value(section, key).strip().lower() == 'true'\n",
    "\n",
    "def get_int_from_ini(section, key):\n",
    "    value = ini_util.get_value(section, key)\n",
    "    return int(value.strip())  # Convert to integer\n",
    "\n",
    "def SampleFromDftrain(dftrain, skip):\n",
    "    # Short time - remove later\n",
    "    if skip:\n",
    "        sample_df = dftrain\n",
    "    else:\n",
    "        shuffled_df = dftrain.sample(frac=1, random_state=42)  # Set a random seed for reproducibility\n",
    "        # Select the first 1000 rows\n",
    "        sample_df = shuffled_df.head(600000)\n",
    "        print('SampleFromDftrain', sample_df.shape)\n",
    "    return sample_df\n",
    "\n",
    "def firebase_init():\n",
    "    # Replace 'path/to/your/serviceAccountKey.json' with the actual path to your JSON file\n",
    "    cred = credentials.Certificate('C:/Users/DELL/Documents/GitHub/ML_Superv_Reg_RandomForest/db17-22f40-firebase-adminsdk-6ko5w-986a994da9.json')\n",
    "    firebase_admin.initialize_app(cred, {'databaseURL': 'https://db17-22f40-default-rtdb.firebaseio.com'})\n",
    "\n",
    "def write_and_get_db(iref, dict):\n",
    "    # Create a reference to the root of the database\n",
    "    # Write a string to the 'messages' node\n",
    "    iref.child('messages').push(dict)\n",
    "    data = iref.get()\n",
    "    print(\"Data retrieved:\", data)\n",
    "\n",
    "# def clearfromdb(iref, keys):\n",
    "#     key1 = '-O1CF0lqKOd6pyN2KdSB'\n",
    "#     key2 = '-O1CFZJg7QtZTk9e-rel'\n",
    "#\n",
    "#     iref.child('messages').child(key1).remove()\n",
    "#     iref.child('messages').child(key2).remove()\n",
    "#\n",
    "def clearfromdb(iref, keys):\n",
    "   for key in keys:\n",
    "         iref.child('messages').child(key).delete()\n",
    "\n",
    "def parse_product_string(product_series):\n",
    "    \"\"\"\n",
    "    Parses a product series where each element is a string.\n",
    "    The function retrieves 4 series: Prod_type, range_min, range_max, and unit.\n",
    "    It also drops the original column 'fiProductClassDesc' from the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        product_series (pd.Series): The input product series.\n",
    "\n",
    "    Returns:\n",
    "        (Prod_type, range_min, range_max, unit) series\n",
    "    \"\"\"\n",
    "    parts = product_series.str.split(\" - \")\n",
    "    Prod_type = parts.str[0]\n",
    "    range_values = parts.str[1].str.split(\" to \")\n",
    "    range_min = pd.to_numeric(range_values.str[0], errors='coerce')\n",
    "    range_max_help = range_values.str[1].str.split()  # Extract numeric part\n",
    "    range_max = range_max_help.str[0]\n",
    "    unit = range_max_help.str[1]\n",
    "    return Prod_type, range_min, range_max, unit\n",
    "\n",
    "\n",
    "def add_additional_columns(df, Prod_type, range_min, range_max, unit):\n",
    "    \"\"\"\n",
    "    Adds 'Prod_type', 'range_min', 'range_max', and 'unit' columns to the DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The existing DataFrame.\n",
    "        Prod_type (str): The value for the 'Prod_type' column.\n",
    "        range_min (float): The value for the 'range_min' column.\n",
    "        range_max (float): The value for the 'range_max' column.\n",
    "        unit (str): The value for the 'unit' column.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the additional columns.\n",
    "    \"\"\"\n",
    "    df['Prod_type'] = Prod_type\n",
    "    df['range_min'] = range_min\n",
    "    df['range_max'] = range_max\n",
    "    df['unit'] = unit\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_and_create_columns(df, columntoextract='fiProductClassDesc', mode=\"validation\"):\n",
    "    df = fiproduct_split_submodels(columntoextract, df)\n",
    "    df = handnum_feature(df)\n",
    "    # Assuming you have a DataFrame 'df' and the categorical column is 'ProductGroup'\n",
    "\n",
    "    # df = target_encode_categorical(df, cat_column='ProductGroup', target_column='SalePrice')\n",
    "    # df = create_interaction_features(df)\n",
    "    # df = generate_polynomial_features(df) - failed\n",
    "    # df = bin_year_into_decades(df)  -failed\n",
    "    # df = log_transform_machine_hours(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def target_encode_categorical(df, cat_column, target_column):\n",
    "    target_means = df.groupby(cat_column)[target_column].mean()\n",
    "    df[f'{cat_column}_encoded'] = df[cat_column].map(target_means)\n",
    "    return df\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    'capture the interaction between the year of manufacture and the minimum range.'\n",
    "    # Example: Multiply 'Feature1' and 'Feature2' to create a new interaction feature\n",
    "    df['InteractionFeature'] = df['YearMade'] * df['range_min']\n",
    "    return df\n",
    "\n",
    "def generate_polynomial_features(df, degree=2):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    poly_features = poly.fit_transform(df[['YearMade', 'range_min']])\n",
    "    poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names(['YearMade', 'range_min']))\n",
    "    df = pd.concat([df, poly_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def bin_year_into_decades(df):\n",
    "    bins = [1980, 1990, 2000, 2010, 2020]  # Adjust the bins as needed\n",
    "    labels = ['1980s', '1990s', '2000s', '2010s']\n",
    "    df['Decade'] = pd.cut(df['YearMade'], bins=bins, labels=labels)\n",
    "    return df\n",
    "\n",
    "def log_transform_machine_hours(df):\n",
    "    df['LogMachineHours'] = np.log1p(df['MachineHoursCurrentMeter'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def handnum_feature(df):\n",
    "    # Assuming 'df' is your DataFrame and 'Sale_Date' is a datetime column\n",
    "    # Convert 'Sale_Date' to datetime if it's not already in that format\n",
    "    df['saleYear_y'] = pd.to_datetime(df['saleYear_y'])\n",
    "\n",
    "    # Group by 'MachineID' and count the number of unique sale dates\n",
    "    hand_num_df = df.groupby('MachineID')['saleYear_y'].nunique().reset_index()\n",
    "\n",
    "    # Merge the 'hand_num_df' back into the original DataFrame\n",
    "    df = pd.merge(df, hand_num_df, on='MachineID', how='left')\n",
    "    df.rename(columns={'Sale_Date_y': 'HandNum'}, inplace=True)\n",
    "\n",
    "    # Display the resulting DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "def fiproduct_split_submodels(columntoextract, df):\n",
    "    # Assuming your original column name is 'fiProductClassDesc'\n",
    "    # Convert 'fiProductClassDesc' to string type if it's not\n",
    "    df[columntoextract] = df[columntoextract].astype(str).str.strip()  # Remove any leading/trailing spaces\n",
    "    Prod_type, range_min, range_max, unit = None, None, None, None\n",
    "    Prod_type, range_min, range_max, unit = parse_product_string(df[columntoextract])\n",
    "    df = add_additional_columns(df, Prod_type, range_min, range_max, unit)\n",
    "    # Assign column names based on the number of columns\n",
    "    if Prod_type is not None and range_min is not None and range_max is not None and unit is not None:\n",
    "        print(\"All variables have non-None values.\")\n",
    "    else:\n",
    "        print(\"Warning: Split did not result in 4 columns. Check the delimiter in 'fiProductClassDesc'.\")\n",
    "    # Drop the original 'fiProductClassDesc' column\n",
    "    df.drop(columns=[columntoextract], inplace=True)\n",
    "    # # Concatenate the split columns back to the original DataFrame\n",
    "    # df = pd.concat([df, split_columns], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_submission_csv(csv_file_path, model, important_categ_column, learn_column, catgeindx, dftrain):\n",
    "    \"\"\"\n",
    "    Generates a submission CSV file with predicted SalePrice based on the provided model.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Path to the input CSV file.\n",
    "        model: Trained machine learning model.\n",
    "\n",
    "    Returns:\n",
    "        None (Creates a CSV file with the submission data).\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "\n",
    "    project_root1 = r\"C:\\Users\\DELL\\Documents\\GitHub\\ML_Superv_Reg_RandomForest\"\n",
    "    valid_file_path1 = os.path.join(project_root1, csv_file_path)\n",
    "    df_valid = pd.read_csv(valid_file_path1)\n",
    "\n",
    "    # Handle the same way as you handled the train CSV data - cleaning, filling, etc.\n",
    "    df_valid = preprocess_and_extract_features(df_valid, important_categ_column, learn_column, \"validation\")\n",
    "    # got dictionary  with small DF\n",
    "    df_dict = df_valid\n",
    "    ind = 0\n",
    "    for df_name, df_valid in df_dict.items():\n",
    "        if catgeindx == df_name:\n",
    "            print(f\"--  {ind}  -- predict_valid started for:{learn_column}\")\n",
    "            df_valid = ColumnsToKeep(df_valid, False)\n",
    "            # Extract relevant features\n",
    "            X_valid = df_valid.set_index('SalesID')\n",
    "            X_valid = df_valid\n",
    "             # Check the shape of df_valid\n",
    "            print(f\"Shape of df_valid: {df_valid.shape}\")\n",
    "\n",
    "            # Check the shape of X_valid\n",
    "            print(f\"Shape of X_valid: {X_valid.shape}\")\n",
    "\n",
    "            y_valid_pred = predict_valid(X_valid, df_valid, model)\n",
    "            submit_csv(y_valid_pred)\n",
    "            return y_valid_pred\n",
    "\n",
    "\n",
    "    # here there is no RMSE. due to that this is the real time data.\n",
    "    # but i can compare to the test value.\n",
    "    # Assuming you have a DataFrame 'df' with columns 'p' and 'x'\n",
    "    #rmse_pandas = ((X_valid[''] - X_valid['x']) ** 2).mean() ** 0.5\n",
    "\n",
    "\n",
    "\n",
    "def submit_csv(y_valid_pred):\n",
    "    # Create a submission CSV file\n",
    "    submission_filename = f'submission_{datetime.now().isoformat()}.csv'\n",
    "    submission_filename = submission_filename.replace(\":\", \"_\")\n",
    "    y_valid_pred.to_csv(submission_filename)\n",
    "    print(f\"Submission CSV file saved as '{submission_filename}'\")\n",
    "\n",
    "\n",
    "def predict_valid(X_valid, df_valid, model):\n",
    "    # Make predictions\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    y_valid_pred = pd.Series(y_valid_pred, index=X_valid.index, name='SalePrice').round(2)\n",
    "    y_valid_pred.index = df_valid['SalesID']\n",
    "    return y_valid_pred\n",
    "\n",
    "\n",
    "def create_date_features(df, mode, df_other=None):\n",
    "    print(f\"\\nCreate Date Features (Operation mode: {mode})\")\n",
    "    df['saledate'] = pd.to_datetime(df['saledate'])\n",
    "\n",
    "    # Add datetime parameters\n",
    "    df['saleYear_y'] = df['saledate'].dt.year\n",
    "    df['saleMonth'] = df['saledate'].dt.month\n",
    "    df['saleDay'] = df['saledate'].dt.day\n",
    "    df['saleDayofweek'] = df['saledate'].dt.dayofweek\n",
    "    df['saleDayofyear'] = df['saledate'].dt.dayofyear\n",
    "\n",
    "    # Drop original saledate\n",
    "    df.drop('saledate', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_unconsistent_rows (df, mode, other_df= None):\n",
    "\n",
    "  print (\"\\nRemove Unconsistent Rows\")\n",
    "  print (\"Operation mode: {mode}\")\n",
    "\n",
    "  # df_Machines_Sorted = df.sort_values(by=['Machine_ID', 'Year_Made','Sale_Date'], ascending=[True, True, True])\n",
    "\n",
    "  # -----------------------------------------------------------------------------\n",
    "\n",
    "  # Group by 'Machine_ID' and check for uniqueness in 'COL1', 'COL2', 'COL3' etc.\n",
    "  # Grouping:\n",
    "  # The DataFrame df is grouped by the Machine_ID column using groupby().\n",
    "\n",
    "  # Aggregation:\n",
    "  # For each group, the agg() function is used to compute the number of unique values (nunique)\n",
    "  # in COL1, COL2, and COL3.\n",
    "\n",
    "  # The result:\n",
    "  # A new DataFrame grouped where each row corresponds to a Machine_ID,\n",
    "  # and the columns contain the count of unique values in each specified column.\n",
    "\n",
    "  if 1!=1:  # mode != 'train':\n",
    "    print ('Execution is not allowed in {mode} mode. allowed only in train mode')\n",
    "    return (df)\n",
    "  else:\n",
    "    print('Removing unconsistent group rows...')\n",
    "\n",
    "    Machine_ID_Grouped_df = df.groupby('MachineID').agg({\n",
    "      'ModelID'                  : 'nunique',\n",
    "      'fiBaseModel'             : 'nunique',\n",
    "      'YearMade'                 : 'nunique',\n",
    "    # 'Fi_Secondary_Desc'         : 'nunique',\n",
    "    # 'Fi_Model_Series'           : 'nunique',\n",
    "    # 'Fi_Model_Descriptor'       : 'nunique',\n",
    "    # 'Product_Size'              : 'nunique',\n",
    "    # 'Fi_Product_Class_Desc'     : 'nunique',\n",
    "    # 'Product_Group'             : 'nunique',\n",
    "      })\n",
    "\n",
    "\n",
    "    # Create a mask where True indicates all values in the group are the same\n",
    "    # Comparison:\n",
    "    # (grouped == 1) creates a boolean DataFrame where each cell is True if the corresponding cell in grouped is equal to 1\n",
    "    # (meaning all values in that column for that Machine_ID are the same).\n",
    "\n",
    "    # Aggregation: .all(axis=1) combines the boolean values across columns.\n",
    "    # For each row (i.e., for each Machine_ID), it returns True if all columns are True\n",
    "    # (i.e., all specified columns have exactly one unique value in that group).\n",
    "\n",
    "    mask = (Machine_ID_Grouped_df == 1).all(axis=1)\n",
    "\n",
    "    # Add the mask to the original DataFrame\n",
    "    # Mapping:\n",
    "    # The map() function maps the boolean mask created for each Machine_ID back to the original DataFrame.\n",
    "    # This adds a new column Mask to df, where each row indicates whether all specified columns have the same value for that Machine_ID.\n",
    "\n",
    "    df['Mask'] = df['MachineID'].map(mask)\n",
    "\n",
    "    # Count the number of True and False values in the 'Mask' column\n",
    "    mask_counts = df['Mask'].value_counts()\n",
    "    print(\"\\nDelete unconsistent rows per Machine ID Grouping\")\n",
    "    print(f\"Number of rows input df:{len(df)}\")\n",
    "    print(f\"Number of False Bad values to FILTER: {mask_counts[False]}\")\n",
    "    print(f\"Number of True Good values to STAY: {mask_counts[True]}\")\n",
    "\n",
    "\n",
    "    # Filtering Out Rows Where the Mask is False\n",
    "    # Boolean Indexing:\n",
    "    # This line uses boolean indexing to filter the DataFrame.\n",
    "    # The condition df['Mask'] returns a boolean Series where the value is True for rows where the Mask column is True and False otherwise.\n",
    "\n",
    "    # Filtering:\n",
    "    # The DataFrame is filtered to include only the rows where the Mask column is True.\n",
    "    # The result is stored in df_filtered.\n",
    "\n",
    "    df_filtered = df[df['Mask']]\n",
    "\n",
    "    # Drop the Mask Column\n",
    "    df_filtered = df_filtered.drop(columns=['Mask'])\n",
    "\n",
    "    print(f\"Number of rows output df:{len(df_filtered)}\")\n",
    "\n",
    "    # df_filtered.head(10)\n",
    "    return (df_filtered)\n",
    "\n",
    "\n",
    "def cleans_tire_size(df, mode, other_df=None):\n",
    "\n",
    "    print(f\"\\nCleans_tire_size\")\n",
    "    print(f\"Operation mode: {mode}\")\n",
    "\n",
    "    # Step 1: Remove the double quotes from the 'Tire_Size' column\n",
    "    print(\"\\nRemoving double quotes from 'Tire_Size' column...\")\n",
    "    df['Tire_Size'] = df['Tire_Size'].str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Step 2: Replace 'None or Unspecified' with NaN\n",
    "    print(\"\\nReplacing 'None or Unspecified' with NaN in 'Tire_Size' column...\")\n",
    "    df['Tire_Size'] = df['Tire_Size'].replace('None or Unspecified', np.nan)\n",
    "\n",
    "    # Step 3: Convert the 'Tire_Size' column to float\n",
    "    print(\"\\nConverting 'Tire_Size' column to float...\")\n",
    "    try:\n",
    "        df['Tire_Size'] = df['Tire_Size'].astype(float)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting 'Tire_Size' to float: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_and_extract_features(dftrain, important_categ_column, learn_column, mode):\n",
    "\n",
    "                 ## Consider to add it on pre analysis ##\n",
    "    # eda_analysis(dftrain, learn_column, important_categ_column, False)\n",
    "                ### Prepare Data  ###\n",
    "\n",
    "                 ### Create extra column for each value in categorial column.###\n",
    "\n",
    "    #dftrain = cleans_tire_size(dftrain, mode)   # error\n",
    "    # dftrain = remove_unconsistent_rows(dftrain, mode)  # error\n",
    "\n",
    "    dftrain = create_date_features(dftrain, mode)\n",
    "    # Training -> all samples up until 2011\n",
    "    dftrain.saleYear_y.value_counts().sort_index()\n",
    "    #(df, exe_dropna=False, exe_dummies=False, exe_exclusenonnumeric=False, exe_missing=False,exe_nonnumeric_code=False)\n",
    "    dftrain = split_and_create_columns(dftrain, 'fiProductClassDesc', mode)\n",
    "    # exe_missing=True, exe_nonnumeric_code=True,\n",
    "\n",
    "    #sign\n",
    "    #(df, exe_missing=False, exe_nonnumeric_code=False, exe_exclusenonnumeric=False,\n",
    "    #exe_dropna=False, exe_dummies=False, print_info=False)\n",
    "\n",
    "    (exe_dropna, exe_dummies, exe_exclusenonnumeric, exe_missing, exe_nonnumeric_code,\n",
    "     exe_FromfilterYear, print_info) = load_from_INI()\n",
    "\n",
    "    dftrain = prepare_data(dftrain, exe_missing, exe_nonnumeric_code, exe_exclusenonnumeric, exe_dropna,\n",
    "                           exe_dummies, exe_FromfilterYear, print_info)\n",
    "\n",
    "    print(\"dftrain.shape\", dftrain.shape)\n",
    "    print(\"Now all of our data is numeric and there are no missing values, we should be able to build a machine\"\n",
    "          \" learning model.\", dftrain.head().T)\n",
    "    # Convert all columns to int64\n",
    "    # df = dftrain.astype(int) error\n",
    "\n",
    "\n",
    "    ## sig: (df, learn_column, clearedcolumn, cnt_std=3, method='sigma', column_with_long_tail='carat', ):\n",
    "    cleareddf = clean_sigma_log(dftrain, learn_column, important_categ_column, 3, 'sigma', important_categ_column, mode)\n",
    "    print(\"cleareddf.shape\", cleareddf.shape)\n",
    "\n",
    "    ## eda_analysis(cleareddf, learn_column, important_categ_column, True)\n",
    "\n",
    "    SubModelPerCat = str(ini_util.get_value('PREPROCESS', 'SubModelPerCat'))\n",
    "    print(cleareddf[SubModelPerCat].unique())\n",
    "\n",
    "    cleareddf_list = {group: sub_df for group, sub_df in cleareddf.groupby('ProductGroupDesc')}\n",
    "    return cleareddf_list\n",
    "\n",
    "\n",
    "def load_from_INI():\n",
    "    exe_missing = get_bool_from_ini('PREPROCESS', 'exe_missing')\n",
    "    exe_nonnumeric_code = get_bool_from_ini('PREPROCESS', 'exe_nonnumeric_code')\n",
    "    exe_exclusenonnumeric = get_bool_from_ini('PREPROCESS', 'exe_exclusenonnumeric')\n",
    "    exe_dropna = get_bool_from_ini('PREPROCESS', 'exe_dropna')\n",
    "    exe_dummies = get_bool_from_ini('PREPROCESS', 'exe_dummies')\n",
    "    print_info = get_bool_from_ini('PREPROCESS', 'print_info')\n",
    "    exe_FromfilterYear = get_int_from_ini('PREPROCESS', 'exe_FromfilterYear')\n",
    "    return (exe_dropna, exe_dummies, exe_exclusenonnumeric, exe_missing, exe_nonnumeric_code, exe_FromfilterYear,\n",
    "            print_info)\n",
    "\n",
    "\n",
    "def load_job():\n",
    "\n",
    "    import joblib\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Create and train your Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_estimators=250, max_features=9)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model to a file\n",
    "    joblib.dump(rf, 'random_forest_model.joblib')\n",
    "\n",
    "    # Later, load the model from the file\n",
    "    loaded_rf = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "    # Now you can use 'loaded_rf' for predictions\n",
    "    preds = loaded_rf.predict(X_test)\n",
    "\n",
    "def createfeatureobefirst():\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Assuming you have X_train and y_train\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = rf.feature_importances_\n",
    "\n",
    "    # Find the index of the desired feature\n",
    "    desired_feature_index = feature_names.index('desired_feature')\n",
    "\n",
    "    # Reorder features (move desired feature to the front)\n",
    "    new_X_train = X_train[:,\n",
    "                  [desired_feature_index] + [i for i in range(X_train.shape[1]) if i != desired_feature_index]]\n",
    "\n",
    "    # Retrain the model with reordered features\n",
    "    rf_reordered = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_reordered.fit(new_X_train, y_train)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Model creation:\n",
    "        A. Pre:\n",
    "            1. feature eng. create columns ,exe_missing, exe_nonnumeric_code,  exe_exclusenonnumeric, exe_dropna, exe_dummies,\n",
    "            2. statitcs\n",
    "            3. outlier handling\n",
    "\n",
    "        B.\n",
    "            1. Model init - Hyper params\n",
    "\n",
    "        C. Post\n",
    "            1. Feature Importance, permutation,  RMSE Test\\Train\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    #SingletonINIUtility.clear()\n",
    "    firebase_init()\n",
    "\n",
    "\n",
    "    # Print the full path and content\n",
    "    print(f\"INI File Path: {ini_file_path}\")\n",
    "    print(\"INI Content:\")\n",
    "    for section in ini_util.config.sections():\n",
    "        print(f\"[{section}]\")\n",
    "        for key, value in ini_util.config.items(section):\n",
    "            print(f\"{key} = {value}\")\n",
    "\n",
    "    learn_column = ini_util.get_value('MODULE', 'learn_column')  # 'tip' #want to learn to predict\n",
    "    important_categ_column = ini_util.get_value('MODULE', 'important_categ_column')    # want to see different distribution on plot\n",
    "    num = ini_util.get_value('TRAIN', 'random_state')\n",
    "    print(f\"learn_column is: {learn_column}\")\n",
    "    print(f\"important_categ_column is: {important_categ_column}\")\n",
    "    print(f\"num is: {num}\")\n",
    "    # when sns i have only train which will be later split- consider to change TODO\n",
    "    #  dftrain will be split to train and Test, dfvalid available only when df comes from url due to BIG data\n",
    "    en = get_bool_from_ini('DATASET', 'url_en')\n",
    "\n",
    "    dftrain, dfvalid = get_df(en)\n",
    "\n",
    "\n",
    "    dftrain.head()\n",
    "    dftrain = SampleFromDftrain(dftrain, False)# remove later\n",
    "\n",
    "    # Instantiate the Random Forest regressor\n",
    "    # rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # # Define hyperparameter distributions\n",
    "    # param_dist = {\n",
    "    #     'n_estimators': randint(50, 200),  # Randomly sample n_estimators\n",
    "    #     'max_depth': randint(5, 20),  # Randomly sample max_depth\n",
    "    #     'min_samples_split': randint(2, 20)  # Randomly sample min_samples_split\n",
    "    # }\n",
    "    #\n",
    "    # # Instantiate Randomized Search\n",
    "    # rf_model = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "    #                                    n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "    #\n",
    "    rf_model = RandomForestRegressor(random_state =get_int_from_ini('TRAIN', 'random_state'),\n",
    "                                     max_depth =get_int_from_ini('TRAIN', 'max_depth'),\n",
    "                                     min_samples_split = get_int_from_ini('TRAIN', 'min_samples_split'),\n",
    "                                     min_samples_leaf = get_int_from_ini('TRAIN', 'min_samples_leaf'),\n",
    "                                     n_estimators= get_int_from_ini('TRAIN', 'n_estimators') ,\n",
    "                                     max_features= get_int_from_ini('TRAIN', 'max_features' ))\n",
    "\n",
    "                                    #max_features=9\n",
    "                                    # max_leaf_nodes: None (unlimited number of leaf nodes)\n",
    "                                    # min_samples_leaf: 1 (minimum number of samples required to be at a leaf node)\n",
    "                                    ## max_leaf_nodes min_samples_leaf\n",
    "                         ### EDA Exploratory  Data analysis ###\n",
    "\n",
    "            # ************&&&&&&&&&&&&&&&&&***********************\n",
    "    cleareddf = preprocess_and_extract_features(dftrain, important_categ_column, learn_column, \"train\")\n",
    "\n",
    "    # ************&&&&&&&&&&&&&&&&&***********************\n",
    "\n",
    "    # Group by 'ProductGroupDesc' and create a dictionary of dataframes with their name of category not numeric\n",
    "\n",
    "    # Initialize an empty list to store models\n",
    "    model_list = {}\n",
    "    list1 = []\n",
    "\n",
    "    for df_name, df in cleareddf.items():\n",
    "        tt = df.copy()\n",
    "        print(f\"look here:\", tt.info())\n",
    "        print(f\"@Cycle {df_name} for category  train creation\")\n",
    "        print(df_name)\n",
    "\n",
    "        df = ColumnsToKeep(df, False, learn_column)  # skip for now\n",
    "\n",
    "        # perform analysis before train is built next row\n",
    "        #eda_analysis(df,learn_column, 'ModelID', True)\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = build_model(rf_model, df, learn_column, False, 1.0)\n",
    "\n",
    "        y_train_pred, y_test_pred = predict_with_model(X_train, X_test, y_train, y_test, rf_model, 1.0, False,\n",
    "                                    df_name)\n",
    "\n",
    "        dftrain.head()\n",
    "        key_model = str(df_name) + '_model'\n",
    "        model_list[str(df_name)] = df_name\n",
    "        model_list[key_model] = rf_model\n",
    "        rt = generate_submission_csv(\"valid.csv\", rf_model, important_categ_column, learn_column,df_name,df)\n",
    "        list1.append(rt)\n",
    "        print(\"--- Cycle categ  small dfs--- \")\n",
    "    # _______________________________________________________________________\n",
    "    ## end ##\n",
    "\n",
    "    # Initialize an empty dataframe to store the combined results\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # list1 has 6 dataframes in the list\n",
    "    for dfi in list1:\n",
    "        # Concatenate the dataframes vertically (along rows)\n",
    "        combined_df = pd.concat([combined_df, dfi])\n",
    "\n",
    "    # Check for duplicated indices\n",
    "    if combined_df.index.duplicated().any():\n",
    "        print(\"Duplicated indices found in the combined DataFrame.\")\n",
    "    else:\n",
    "        print(\"No duplicated indices in the combined DataFrame.\")\n",
    "\n",
    "    # Calculate the average for corresponding values\n",
    "    combined_df = combined_df.groupby(combined_df.index).mean()\n",
    "\n",
    "    # Assuming your DataFrame is named 'combined_df'\n",
    "    combined_df = combined_df.rename_axis('SalesID')\n",
    "\n",
    "    # Reset the index to create a new default integer index\n",
    "    #combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    print(combined_df)\n",
    "\n",
    "\n",
    "    submit_csv(combined_df)\n",
    "    print(\"--- END Run Good Bye--- \")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust the path based on your project location\n",
    "    project_root = r\"C:\\Users\\DELL\\Documents\\GitHub\\ML_Superv_Reg_RandomForest\"\n",
    "    ini_file_name = \"config.INI\"\n",
    "    ini_file_path = os.path.join(project_root, ini_file_name)\n",
    "    # Create an instance of SingletonINIUtility\n",
    "    ini_util = SingletonINIUtility(ini_file_path)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d40ab9-d454-4610-9f82-13237df40f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
